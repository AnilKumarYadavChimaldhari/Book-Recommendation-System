{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team members: Deepak Maran, Kewei Liu, Rakshita Nagalla, Xiaohui Guo**  \n",
    "\n",
    "Part I of the project for E4571 Personalization: Theory & Application.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better recommendations a User satisfaction a Retention of users and brand value.  \n",
    "\n",
    "While T-shirts, promotions and partnerships with book publishers have generated some income, BookCrossing's best source of revenue has proven to be \"release kits\" that provide members with the eye-catching labels they need to attract a passerby's attention to their books. Membership is free, as are more rudimentary labels, but these kits that start at $18 apiece have proven to be a hit.  \n",
    "\n",
    "\"We get 400 new members a day who are pretty excited about joining BookCrossing, and half of our sales go to new members on day zero,\" he says. \"With a quarter-million people it doesn't take a very large percentage to be buying things to keep a small team like ours in the black.\"  \n",
    "\n",
    "Two years ago, Hornbaker told me that only 10% of released books were ever heard from again, a reality that tended to deflate the interest of early participants. Today that rate is up to between 20% and 25%, an increase Hornbaker attributes to heavy press coverage and good word of mouth.  \n",
    "\n",
    "A good book recommendation system can be used for Marketing and advertising, partnerships with publishers, release kits, and wings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of our project:\n",
    "- Better personalized recommendations based on location of user and available books helps build brand (Normal recommendation system, not top-k).  \n",
    "- Top-k personalized ranking for purchase from Better World Books.  \n",
    "\n",
    "Implementation:\n",
    "- Notify user if a book she might like is wild-released near her location (recommendation based on predicted user rating and location).  \n",
    "- Give user top-k recommendations for purchase from Better World Books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "Book crossing is a website that enables free sharing of books, either with friends or with complete strangers by wild-releasing books in public places.  \n",
    "\n",
    "No. of ratings, No. of users, no. of items.  \n",
    "\n",
    "Very sparse dataset. Here, the density of the dataset means the percentage of user-book pairs that have a rating. It is calculated as :\n",
    "\n",
    "$$Density = \\frac{no.of\\, ratings}{no.of\\, users * no.of\\, items}$$\n",
    "\n",
    "It is an explicit feedback dataset where users explicitly rate books on a scale of 1-10. We assume that this ratings data is a proxy of the actual preference of the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "1. Cleaning:   \n",
    "    Since the dataset is very sparse and we need to perform a top-10 recommendation, we only work with users who have rated atleast 20 items and which were rated at least 10 times. The sparsity of the dataset then becomes XXXX. At this stage, the dataset contain XXXX users, YYYY items and ZZZZ items. The\n",
    "2. Splitting:  \n",
    "    We put aside YY% of the data for testing. This test dataset was formed by selecting 10 random ratings for each user in the dataset. This ensures that there are a fair number of ratings for recommending top-10 books to the user. The remaining data is then split into train and validation sets using a 4-fold cross-validation and hyperparameters were tuned. Once the tuning is complete, we train on the whole training+validation set and predict on the held-out test dataset(Note that this test set was not used in hyperparameter training to ensure unbiased evaluation of the algorithms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objective is to recommend $k$ items to the user, evaluation metrics like RMSE and MAE which measure accuracy of rating prediction are not very indicative of the end-user experience. To satisfy our business objective, we are are more concerned about the precision at the top. Hence we choose the following evaluation metrics: \n",
    "- **Precision**:   \n",
    "    Precision measures the proportion of recommended items that are actually relevant and is directly related to our business objective. We define an item as being relevant to a user, if he gave it a rating of 7 or above. We empirically chose the threshold value to be 7 based on the ratings distribution.  \n",
    "\n",
    "- **Recall**:   \n",
    "    Recall indicates the proportion of relevant items that are in the recommended list. We consider recall also because it provides information on the expected performance of the recommender system regarding the unknown complete data (i.e., all books), which exactly is what is experienced by the user. If we assume that relevant ratings are missing at random (while allowing all other rating values to be missing not at random), then Recall can be estimated without bias from observed MNAR data.  \n",
    "\n",
    "- **F-score**:   \n",
    "    F-score summarizes the trade-off between precision and recall.  \n",
    "$$F\\, score = \\frac{2 * Precision * Recall}{Precision + Recall}$$\n",
    "  \n",
    "- **NDCG**:   \n",
    "    In our case, the ranking between 1st and 2nd ranked books is more important than those at the end of the list. Moreover, unlike in precision and recall, NCDG does not require relevance to be binary, allowing the user’s preference of the book to expressed in multiple levels of relevance. In order to incorporate these idea, we use the normalized discounted cumulative gain to evaluation our recommendation lists.  \n",
    "[insert formula from slide]  \n",
    "\n",
    "Note that, we choose $rel_{u,j}$ to be the actual rating of the user $u$ gave to item $j$.\n",
    "\n",
    "We also plot the precision-recall curves for all the three algorithms for different values of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Neighborhood-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert short description of the method]\n",
    "Memory based  \n",
    "Pros:  \n",
    "- Simple, good enough, can build social links too, incremental  \n",
    "\n",
    "Cons:  \n",
    "- Sparse data is a problem (how to calculate similarity?)\n",
    "- Users are in reality interested in a mixture of topics, and very few users are interested in exactly the same mixture, so basing everything on similarity is Simplistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Method Selecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by using the k-nearest neighbour method due to its simplicity and intuitiveness. The following variations were tried with kNN:   \n",
    "1. Basic kNN, kNN with mean, KNN with Z-score  \n",
    "2. Similarity: Cosine, Pearson, Pearson_baseline  \n",
    "3. User-based, item-based   \n",
    "The approach of taking item-based similarity is seen to perform better than user-based similarity. The search for users in a large user population is the bottleneck in user-based methods. This is overcome by instead exploring relationships between items first which are smaller in number.\n",
    "A basic item-based kNN model with the Pearson similarity is seen to work best for our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the overall recommendation accuracy of the item-based algorithms does tend to improve as we increase the value of k?\n",
    "\n",
    " increasing the value of k does not lead to significant improvements. This is particularly important since small values of k lead to fast recommendation rates (i.e., low computational requirements) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Model-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the SVD-based Matrix factorization technique to recommend books to users. While SVD is not best-suited for sparse matrices, we choose this model due to its simplicity and effectiveness when using explicit data. Other methods like ALS are not very effective in case of large explicit ratings matrices.   \n",
    "\n",
    "Instead of assuming that a user $u$’s rating for item $i$ can be described simply by the dot product of the user and item latent vectors, we will consider that each user and book can have a bias term associated with them. The rationale is that certain users might not be very critical and tend to rate all books highly, or certain books may tend to always have low ratings. We also include a global bias term. These biases are learnt by the model.  \n",
    "\n",
    "We must observe that our ratings matrix is very sparse even after the basic cleaning (with a density of XXXX). Since the observed ratings are so few, the model is prone to overfit. We overcome this problem by adding regularization terms to the objective function.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MF assumes that each user can be represented by $k$ attributes (latent vectors) which are unknown to us. We pick a $k$ that gives the most accurate results.\n",
    "\n",
    "[Plot of test and training ndcg vs number of factors]\n",
    "\n",
    "As discussed, we choose a regularization parameter which accounts for the overfitting in the model. While the regularization parameter ($\\lambda>0$) can be different for each of the latent factors and bias terms, we choose it be the same to make tuning simpler.\n",
    "\n",
    "[Plot of test and training rmse vs regularization parameter]\n",
    "\n",
    "Also tune for learning rate.\n",
    "\n",
    "[Plot of test and training rmse vs learning rate]\n",
    "\n",
    "**Results:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing these results, what other design choices might you consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration on Sample Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Accuracy vs Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run-time vs Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do these methods meet your hypothetical objectives?   \n",
    "Would you feel comfortable putting these solutions into production at a real company?   \n",
    "potential watch outs?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.networkworld.com/article/2322697/software/bookcrossing-com--chapter-2.html  \n",
    "http://www.bookcrossing.com/faqs  \n",
    "https://www.betterworldbooks.com/go/bwb-bookcrossing  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
